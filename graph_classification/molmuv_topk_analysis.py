import os
import json
import torch
import numpy as np
import argparse
import re
from tqdm import tqdm
from sklearn.metrics import ndcg_score
from torch_geometric.loader import DataLoader
from ogb.graphproppred import PygGraphPropPredDataset

# Import your model architectures
from gnn_models import GCN, GAT, SAGE, GIN

def parse_config(param_str):
    """
    Robustly extracts hyperparams from folder names generated by generate_param_string.
    Example: lr-0.0001_pool-mean_metric-rocauc_hid-64_do-0.1_layers-5_bn-True_res-True...
    """
    hid = re.search(r'hid-(\d+)', param_str)
    layers = re.search(r'layers-(\d+)', param_str)
    bn = re.search(r'bn-(True|False)', param_str)
    res = re.search(r'res-(True|False)', param_str)
    dropout = re.search(r'do-([\d\.]+)', param_str)
    pool = re.search(r'pool-([a-z]+)', param_str)

    if hid and layers:
        return {
            'hid': int(hid.group(1)),
            'layers': int(layers.group(1)),
            'bn': bn.group(1) == 'True' if bn else False,
            'res': res.group(1) == 'True' if res else False,
            'dropout': float(dropout.group(1)) if dropout else 0.0,
            'pool': pool.group(1) if pool else 'mean'
        }
    return None

def compute_molmuv_topk(y_true, y_pred, k_percents=None):
    # FIXED: Syntax error resolved
    if k_percents is None:
        k_percents = [0.5, 1, 2, 3, 4, 5] + list(range(10, 110, 10))
    
    num_tasks = y_true.shape[1]
    task_ndcgs = {k: [] for k in k_percents}
    
    # Sanity Check: Warn if predictions have zero variance (model collapse)
    if np.std(y_pred) < 1e-6:
        print("WARNING: Model predictions have zero variance! nDCG will be 0.")

    for t in range(num_tasks):
        # Mask logic: only consider valid labels (0 or 1), ignore -1
        mask = y_true[:, t] != -1
        
        # Check if we have enough data for ranking
        y_t_masked = y_true[mask, t]
        
        # We need at least 1 positive and 1 negative to calculate meaningful ranking metrics
        if len(y_t_masked) < 2 or (y_t_masked == 1).sum() == 0:
            continue
            
        y_t = y_t_masked.reshape(1, -1)
        p_t = y_pred[mask, t].reshape(1, -1)
        
        n_samples = y_t.shape[1]
        for k_pct in k_percents:
            # Calculate absolute K based on percentage of valid samples for this task
            k_abs = max(1, int(n_samples * (k_pct / 100.0)))
            try:
                # ndcg_score expects (y_true, y_score)
                score = ndcg_score(y_t, p_t, k=k_abs)
                task_ndcgs[k_pct].append(score)
            except Exception:
                continue

    # Average across valid tasks
    summary = {}
    for k, scores in task_ndcgs.items():
        summary[f"ndcg_at_{k}_pct_mean"] = float(np.nanmean(scores)) if scores else 0.0
    return summary

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--base_dir", type=str, default="results_weighted/ogbg-molmuv")
    parser.add_argument("--device", type=int, default=0)
    args = parser.parse_args()

    device = torch.device(f"cuda:{args.device}" if torch.cuda.is_available() else "cpu")
    
    print("Loading Dataset...")
    # Load dataset
    try:
        dataset = PygGraphPropPredDataset(name='ogbg-molmuv', root='data/OGB')
    except Exception as e:
        print(f"Error loading dataset: {e}")
        return

    # --- CRITICAL: Replicate Training Script Preprocessing ---
    # Convert NaN to -1. Without this, masking fails.
    data_obj = dataset.data
    if hasattr(data_obj, 'y'):
        y_temp = data_obj.y
        if torch.is_floating_point(y_temp) and torch.isnan(y_temp).any():
            print("Preprocessing: Converting NaNs to -1...")
            y_temp = y_temp.clone()
            y_temp[torch.isnan(y_temp)] = -1
            dataset.data.y = y_temp
    # ---------------------------------------------------------

    split_idx = dataset.get_idx_split()
    test_loader = DataLoader(dataset[split_idx['test']], batch_size=128, shuffle=False)

    for model_name in os.listdir(args.base_dir):
        model_path = os.path.join(args.base_dir, model_name)
        if not os.path.isdir(model_path): continue
        
        # Skip if model is not a GNN (e.g. if it's subgraphormer/gps which need special transforms)
        if model_name.lower() not in ['gcn', 'gat', 'sage', 'gin']:
            continue

        for config_str in os.listdir(model_path):
            config_path = os.path.join(model_path, config_str)
            if not os.path.isdir(config_path) or config_str == 'summary': continue

            cfg = parse_config(config_str)
            if not cfg: 
                # print(f"Skipping unrecognized config: {config_str}")
                continue

            topk_dir = os.path.join(config_path, "topk_results")
            os.makedirs(topk_dir, exist_ok=True)
            
            run_folders = [d for d in os.listdir(config_path) if d.startswith('run_')]
            all_run_metrics = []

            for run_id in tqdm(run_folders, desc=f"Eval {model_name} {config_str[:20]}"):
                ckpt = os.path.join(config_path, run_id, "model.pt")
                if not os.path.exists(ckpt): continue

                # Model Initialization
                models = {'gcn': GCN, 'gat': GAT, 'sage': SAGE, 'gin': GIN}
                ModelClass = models.get(model_name.lower())
                
                try:
                    # Match training initialization: use_ogb_features=True
                    model = ModelClass(
                        num_features=dataset.num_features,
                        num_classes=dataset.num_tasks,
                        hidden=cfg['hid'],
                        num_layers=cfg['layers'],
                        dropout=cfg['dropout'],
                        pool=cfg['pool'],
                        use_ogb_features=True, 
                        use_bn=cfg['bn'],
                        use_residual=cfg['res']
                    )
                    
                    model.load_state_dict(torch.load(ckpt, map_location=device))
                    model.to(device).eval()
                except Exception as e:
                    print(f"\nError loading {run_id}: {e}")
                    continue

                preds, ys = [], []
                with torch.no_grad():
                    for data in test_loader:
                        out = model(data.to(device))
                        preds.append(out.cpu())
                        ys.append(data.y.cpu())
                
                # Check shapes
                y_true_all = torch.cat(ys).numpy()
                y_pred_all = torch.cat(preds).numpy()
                
                metrics = compute_molmuv_topk(y_true_all, y_pred_all)
                all_run_metrics.append(metrics)

                with open(os.path.join(topk_dir, f"{run_id}_metrics.json"), 'w') as f:
                    json.dump(metrics, f, indent=2)

            if all_run_metrics:
                summary = {k: float(np.mean([m[k] for m in all_run_metrics])) for k in all_run_metrics[0]}
                with open(os.path.join(topk_dir, "summary.json"), 'w') as f:
                    json.dump(summary, f, indent=2)

if __name__ == "__main__":
    main()